# PrivateInvestigator

### Complexity & general exmplentation:
Let n be the number of sentences, and m the average amount of words in a sentence.
I will go step by step of the program:
1) The SentenceReader passes on all the lines in the file and convert them to Sentences, when the last word of a sentence is always "\n" - time complexity is O(n)
2) Then each Sentence is added to the WordsGraph - time complexity is O(n*m).
In  WordsGraph, the Sentences are stored by reference in two different data structures:
- A HashSet called allSentences
- A HashMap called wordsByPosition, that contains keys for all the indexes possible within a sentence (AKA the number of words in the sentence, which is approximately m). The values of this HashMap, represented by another type of HashMap (sentencesByWord), and the keys of this kind is all the words that ever appeared at this index in some sentence. The values of the second HashMap  (sentencesByWord) are lists with all the Sentences that have the chosen word at the chosen index. For example, the first HashMap (wordsByPosition) contains a "1" as a key, and the value is a HashMap with the keys "Naomi" and "George". The key "Naomi" points to a list of Sentences that has "Naomi" as their first word, and the "George" key point to a list of Sentences that have "George" as their first word.
Reviewing my code, this is something I would have done differently: I think there is no point of doing this 2-level HashMap. Instead, I would have created one HashMap that has a key which is a combination of a word and an index. In both of the approaches the time complexity of the lookup of a specific word in a specific index is O(1), the only "advantage" of the first approach is that it can give me ALL the possible words in a specific index by O(1) - but I don't need that for my algorithm. In both of the cases the space complexity is bounded by O(n*m) but I do handle more HashMaps in the first approach, and since every HashMap has a "wasted" space of unused cells - I do use more space.
I think I would have created one HashMap with the second approach, and if I ever needed to fetch all words that are possible in a certain index, or all indexes possible for a certain word - I would create a relevant HashMap for user case, with values that are pointers for the elementary HashMap.
3) Then I call WordsGraphUtils that passes over all the sentences (n), and for each sentence s1 it passes through all it's words (O(m)). For each word, I look for the sentences that has the same word at the same position - let us call this  group S2. The lookup of this group is done by O(1), and the size of it is bounded by n (though it's probably less, but that's the worst case). Then I pass through all the sentences in S2 (O(n)) and see if they are relevant for me (not too different from s1). I keep track of all the sentences from S2 that are still relevant (space complexity bounded by n) and go on to the next word in s1. When I finish to go through all the words in s1 (including the END_OF_SENTENCE word) I go through all the relevant Sentences again (which is, more or less, the intersection of S2 groups from all the iterations), and remove again the irrelevant Sentences O(n). Then I print all the relevant Sentences similar to s1 (the ignoreDuplicates parameter keeps me from printing s1 and s2 and then s2 and s1).
In total, the time complexity of this step is bounded by O(n^2*m), and the space complexity by O(n).
4) In total, for all steps, time complexity is O(n^2*m) and space complexity is O(n*m).

### How I would scale my algorithm / what I could do within more time:
1) Instead of keeping word as a String, I will keep it as a Word entity. This entity will have an id and many possible values. For example, I can store in the word "walk" with several values: "walks", "walking", "to walk", etc. This is how I can be less strict about the similar sentences I can group together. Before creating a new Word, I will try to lookup for a relevant one from the DB through a dedicated service. If the exact Word already exists, I will get its id. Else, I will try to find Words with similar values and consider adding the new value to one of them, and return its id. If nothing like that existed before, I will create a new one and return the id. A Sentence will be composed of Date and an ArrayList of Words.
2) I will probably use Spring in order to manage all the classes.
3) I will change the strategy - instead of allowing only one word to be missing, I will allow more than one. This can be done by keeping scoring alongside the sentences from  S2, and after passing over all the words of s1, choosing the sentences that passed the wanted threshold of scoring.
4) I will create dedicated exception classes.
5) I can use threads and pass over several s1 sentences in parallel (Since Sentence is immutable, I don't see any reason to convert it's inner ArrayList (asynchronous) to Vector (synchronous)).
6) I will read about ElasticSeach and see if it already provides a more efficient way to search in sentences.
7) I will change the 2-levels HashMap to 1-level HashMap, as denoted in step 2 of the previous section.
